{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>I'd like to puts some CD-ROMS on my iPad, is that possible?' — Yes, but wouldn't that block the screen?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>My ipod is officially dead. I lost all my pictures and videos from the 1D and 5sos concert,and from Vet Camp #hatinglife #sobbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>Been fighting iTunes all night! I only want the music I $&amp;@*# paid for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  7921   \n",
       "1  7922   \n",
       "2  7923   \n",
       "3  7924   \n",
       "4  7925   \n",
       "\n",
       "                                                                                                                               tweet  \n",
       "0                                                      I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks  \n",
       "1                currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/  \n",
       "2                          I'd like to puts some CD-ROMS on my iPad, is that possible?' — Yes, but wouldn't that block the screen?\\n  \n",
       "3  My ipod is officially dead. I lost all my pictures and videos from the 1D and 5sos concert,and from Vet Camp #hatinglife #sobbing  \n",
       "4                                                             Been fighting iTunes all night! I only want the music I $&@*# paid for  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = pd.read_csv('test.csv')\n",
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removeAtMention(tweet):\n",
    "    tweetAfterRemovingAt = [re.sub(r'^@\\w*','',eachword) for eachword in tweet.split()]\n",
    "    tweetSentence = ' '.join(tweetAfterRemovingAt)\n",
    "    return tweetSentence.strip()\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(removeAtMention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHashTags(tweet):\n",
    "    tweetAfterRemovingHash = [re.sub(r'^#','',eachword) if eachword.startswith('#') \n",
    "                                                        else eachword for eachword in tweet.split()]\n",
    "    tweetSentence = ' '.join(tweetAfterRemovingHash)\n",
    "    return tweetSentence.strip()\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(removeHashTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUrl(text):\n",
    "    \"\"\"\n",
    "    Remove urls from text\n",
    "    Example: link to latest cricket score. https://xyz.com/a/b => link to latest cricket score.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed urls\n",
    "    \"\"\"\n",
    "    flag=0\n",
    "    urlfree = []\n",
    "    for word in text.split():\n",
    "        if not (word.startswith(\"www\") or word.startswith(\"http\") or \n",
    "                word.endswith(\".html\") or re.search('.com',word)):\n",
    "            urlfree.append(word)\n",
    "    urlfree = \" \".join(urlfree)\n",
    "\n",
    "    urls = re.finditer(r'http[\\w]*:\\/\\/[\\w]*\\.?[\\w-]+\\.+[\\w]+[\\/\\w]+', urlfree)\n",
    "    for i in urls:\n",
    "        urlfree = re.sub(i.group().strip(), '', urlfree)\n",
    "    return urlfree\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(removeUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIsVulgar(sentence):\n",
    "    words = sentence.split()\n",
    "    if '$&@*#' in words:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "testData['isVulgar'] = testData['tweet'].apply(getIsVulgar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceVulgarPattern(sentence):\n",
    "    words = sentence.split()\n",
    "    wordsReplaced = [eachword if eachword != '$&@*#' else 'bad' for eachword in words]\n",
    "    sentenceReplaced = ' '.join(wordsReplaced)\n",
    "    return sentenceReplaced\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(replaceVulgarPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMiddleVulgarPattern(sentence):\n",
    "    wordsReplaced = [re.sub(r'\\$\\&\\@\\*\\#',' wrong ',eachword) for eachword in sentence.split()]\n",
    "    sentenceReplaced = ' '.join(wordsReplaced)\n",
    "    return sentenceReplaced\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(replaceMiddleVulgarPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emo import emo\n",
    "def emoticonsScore(tweet):\n",
    "    words = tweet.split()\n",
    "    emoScore = 0\n",
    "    for word in words:\n",
    "        if word in emo:\n",
    "            emoScore += emo[word]\n",
    "    return emoScore\n",
    "\n",
    "testData['emoScore'] = testData['tweet'].apply(emoticonsScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\.\\%\\[\\]\\<]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "\n",
    "def replaceRepeatedChar(tweet):\n",
    "    replacer = RepeatReplacer()\n",
    "    tweetAfterReplacement = [replacer.replace(eachword) for eachword in tweet.split()]\n",
    "    tweetSentence = ' '.join(tweetAfterReplacement)\n",
    "    return tweetSentence.strip()\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(replaceRepeatedChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def separate_digit_text(text):\n",
    "    \"\"\"\n",
    "    Separate digit and words with space in text\n",
    "    Example: I will be booking tickets for 2adults => I will be booking tickets for 2 adults   \n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with separated digits and words\n",
    "    \"\"\"\n",
    "    regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
    "    clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
    "    return clean_text\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(separate_digit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slangs import slangs_dict\n",
    "\n",
    "def slang_look_up(text):\n",
    "    \"\"\"\n",
    "    Replace slang word in text to their original form\n",
    "    Example: hi, thanq so mch => hi, thank you so much\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        slanged (str): cleaned text with replaced slang\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in slangs_dict:\n",
    "            new_text.append(slangs_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    slanged = \" \".join(new_text)\n",
    "    return slanged\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(slang_look_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from appos import appos_dict\n",
    "\n",
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(appos_look_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits_with_char(text, replace_char=' digit '):\n",
    "    \"\"\"\n",
    "    Replace digits to `replace_char`\n",
    "    Example: I will be there on 22 april. => I will be there on dd april.\n",
    "    Args:\n",
    "        text (str): text\n",
    "        replace_char (str): character with which digit has to be replaced\n",
    "    Returns:\n",
    "        clean_text (str): clean text with replaced char for digits\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[0-9]+')\n",
    "    clean_text = regex_pattern.sub(replace_char, text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(replace_digits_with_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sb_stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "pt_stem = PorterStemmer()\n",
    "\n",
    "def stem_text(text, stemmer='snowball'):\n",
    "    \"\"\"\n",
    "    Convert words in text into their root form\n",
    "    Example: I am playing in ground => I am play in ground \n",
    "    Args:\n",
    "        text (str): text\n",
    "        \n",
    "    Returns:\n",
    "        text_stem (str): cleaned text with replaced stem words\n",
    "    \"\"\"\n",
    "#     text = remove_inside_braces(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stemmer == 'snowball':\n",
    "        text_stem = \" \".join([sb_stem.stem(w) for w in tokens])\n",
    "    else:\n",
    "        text_stem = \" \".join([pt_stem.stem(w) for w in tokens])\n",
    "    \n",
    "    return text_stem\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAscii(tweet):\n",
    "    #For Removing non ascii characters \n",
    "    sentenceClean = ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    sentenceClean = ' '.join([eachWord.strip() for eachWord in sentenceClean.split()])\n",
    "    return sentenceClean\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(removeNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMorePunctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\<\\*\\#\\$\\>\\@\\&\\'\\~\\_\\/\\;\\=\\%]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text\n",
    "\n",
    "testData['tweet'] = testData['tweet'].apply(removeMorePunctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#SamsungGalaxyS9'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_digit_text('#SamsungGalaxyS9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.to_csv('CleanDataTest.csv',index=None,columns=['id','tweet','isVulgar','emoScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
